#!/usr/bin/env python3
"""é‡æ„æ•ˆæœéªŒè¯æµ‹è¯•

éªŒè¯ç­–ç•¥æ¨¡å¼é‡æ„å’Œç®¡é“å¤„ç†ä¼˜åŒ–çš„æ•ˆæœã€‚
æµ‹è¯•é‡æ„åçš„è°ƒåº¦å™¨å’Œç®¡é“æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œã€‚
"""

import sys
import os
import time
import tempfile
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from rayinfo_backend.collectors.base import (
    RawEvent,
    BaseCollector,
)
from rayinfo_backend.scheduling.scheduler import SchedulerAdapter
from rayinfo_backend.pipelines import DedupStage, PersistStage, Pipeline
from rayinfo_backend.config.loaders import create_default_config_loader, ConfigParser
from rayinfo_backend.config.validators import validate_settings
from rayinfo_backend.utils.instance_id import instance_manager, InstanceStatus


class TestSimpleCollector(BaseCollector):
    """æµ‹è¯•ç”¨çš„ç®€å•é‡‡é›†å™¨"""

    name = "test.simple"

    @property
    def default_interval_seconds(self) -> int:
        return 5

    async def fetch(self, param=None):
        # æ¨¡æ‹Ÿé‡‡é›†æ•°æ®
        for i in range(3):
            yield RawEvent(
                source=self.name,
                raw={"post_id": f"simple_{i}", "content": f"Simple content {i}"},
                debug=True,  # æµ‹è¯•æ¨¡å¼
            )


class TestParameterizedCollector(BaseCollector):
    """æµ‹è¯•ç”¨çš„å‚æ•°åŒ–é‡‡é›†å™¨"""

    name = "test.parameterized"

    @property
    def default_interval_seconds(self) -> int:
        return 10

    def list_param_jobs(self):
        return [
            ("query1", 5),
            ("query2", 8),
        ]

    async def fetch(self, param=None):
        if param is None:
            return

        # æ¨¡æ‹Ÿå‚æ•°åŒ–é‡‡é›†
        for i in range(2):
            yield RawEvent(
                source=self.name,
                raw={
                    "post_id": f"{param}_{i}",
                    "query": param,
                    "content": f"Content for {param} - {i}",
                },
                debug=True,  # æµ‹è¯•æ¨¡å¼
            )


def test_scheduler_strategy_pattern():
    """æµ‹è¯•è°ƒåº¦å™¨ç­–ç•¥æ¨¡å¼é‡æ„"""
    print("=== æµ‹è¯•è°ƒåº¦å™¨ç­–ç•¥æ¨¡å¼é‡æ„ ===")

    # åˆ›å»ºä¸´æ—¶æ•°æ®åº“æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp_db:
        db_path = tmp_db.name

    try:
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
        os.environ["RAYINFO_DB_PATH"] = db_path

        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = SchedulerAdapter()

        # åˆ›å»ºæµ‹è¯•é‡‡é›†å™¨
        simple_collector = TestSimpleCollector()
        param_collector = TestParameterizedCollector()

        # æµ‹è¯•æ·»åŠ ç®€å•é‡‡é›†å™¨
        job_ids = scheduler.add_collector_job(simple_collector)
        print(f"âœ“ ç®€å•é‡‡é›†å™¨æ·»åŠ æˆåŠŸï¼Œä»»åŠ¡ID: {job_ids}")
        assert len(job_ids) == 1
        assert job_ids[0] == "test.simple"

        # æµ‹è¯•æ·»åŠ å‚æ•°åŒ–é‡‡é›†å™¨
        job_ids = scheduler.add_collector_job(param_collector)
        print(f"âœ“ å‚æ•°åŒ–é‡‡é›†å™¨æ·»åŠ æˆåŠŸï¼Œä»»åŠ¡ID: {job_ids}")
        assert len(job_ids) == 2
        assert "test.parameterized:query1" in job_ids
        assert "test.parameterized:query2" in job_ids

        print("âœ“ è°ƒåº¦å™¨ç­–ç•¥æ¨¡å¼é‡æ„éªŒè¯é€šè¿‡")

    except Exception as e:
        print(f"âœ— è°ƒåº¦å™¨ç­–ç•¥æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        raise
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(db_path):
            os.unlink(db_path)


def test_pipeline_optimization():
    """æµ‹è¯•ç®¡é“å¤„ç†ä¼˜åŒ–"""
    print("\n=== æµ‹è¯•ç®¡é“å¤„ç†ä¼˜åŒ– ===")

    try:
        # åˆ›å»ºæµ‹è¯•äº‹ä»¶
        events = [
            RawEvent("test.source", {"post_id": "1", "content": "Content 1"}),
            RawEvent("test.source", {"post_id": "2", "content": "Content 2"}),
            RawEvent(
                "test.source", {"post_id": "1", "content": "Content 1 duplicate"}
            ),  # é‡å¤
            RawEvent("test.source", {"post_id": "3", "content": "Content 3"}),
            RawEvent(
                "test.source", {"post_id": "2", "content": "Content 2 duplicate"}
            ),  # é‡å¤
        ]

        # æµ‹è¯•å»é‡é˜¶æ®µ
        dedup_stage = DedupStage(max_size=1000)
        unique_events = dedup_stage.process(events)

        print(
            f"âœ“ å»é‡æµ‹è¯•ï¼šè¾“å…¥ {len(events)} ä¸ªäº‹ä»¶ï¼Œè¾“å‡º {len(unique_events)} ä¸ªäº‹ä»¶"
        )
        assert len(unique_events) == 3  # åº”è¯¥å»é™¤2ä¸ªé‡å¤é¡¹

        # æ£€æŸ¥å»é‡æŒ‡æ ‡
        metrics = dedup_stage.get_metrics()
        print(f"âœ“ å»é‡æŒ‡æ ‡ï¼š{metrics}")
        assert metrics["total_input"] == 5
        assert metrics["duplicates_found"] == 2
        assert metrics["dedup_rate"] == 0.4

        # æµ‹è¯•ç®¡é“ç»„åˆ
        pipeline = Pipeline(
            [DedupStage(max_size=100), PersistStage()]  # æµ‹è¯•é˜¶æ®µï¼Œåªæ‰“å°
        )

        result = pipeline.run(events)
        print(f"âœ“ ç®¡é“å¤„ç†å®Œæˆï¼Œç»“æœæ•°é‡: {len(result)}")

        print("âœ“ ç®¡é“å¤„ç†ä¼˜åŒ–éªŒè¯é€šè¿‡")

    except Exception as e:
        print(f"âœ— ç®¡é“å¤„ç†ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        raise


def test_config_management():
    """æµ‹è¯•é…ç½®ç®¡ç†é‡æ„"""
    print("\n=== æµ‹è¯•é…ç½®ç®¡ç†é‡æ„ ===")

    try:
        # æµ‹è¯•é…ç½®åŠ è½½å™¨
        loader = create_default_config_loader()
        config_data = loader.load()
        print(f"âœ“ é…ç½®åŠ è½½å™¨æµ‹è¯•é€šè¿‡ï¼ŒåŠ è½½äº† {len(config_data)} ä¸ªé…ç½®é¡¹")

        # æµ‹è¯•é…ç½®è§£æ
        settings = ConfigParser.parse(config_data)
        print(f"âœ“ é…ç½®è§£ææˆåŠŸï¼Œæ—¶åŒº: {settings.scheduler_timezone}")

        # æµ‹è¯•é…ç½®éªŒè¯
        validation_result = validate_settings(settings)
        print(f"âœ“ é…ç½®éªŒè¯å®Œæˆï¼Œæ˜¯å¦æœ‰æ•ˆ: {validation_result.is_valid}")
        print(f"  - é”™è¯¯æ•°: {len(validation_result.errors)}")
        print(f"  - è­¦å‘Šæ•°: {len(validation_result.warnings)}")

        print("âœ“ é…ç½®ç®¡ç†é‡æ„éªŒè¯é€šè¿‡")

    except Exception as e:
        print(f"âœ— é…ç½®ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        raise


def test_instance_manager():
    """æµ‹è¯•å®ä¾‹ç®¡ç†å™¨å¢å¼º"""
    print("\n=== æµ‹è¯•å®ä¾‹ç®¡ç†å™¨å¢å¼º ===")

    try:
        # æ¸…ç†ç°æœ‰å®ä¾‹
        instance_manager.clear()

        # æµ‹è¯•å®ä¾‹æ³¨å†Œ
        collector = TestSimpleCollector()
        instance_id = instance_manager.register_instance(collector)
        print(f"âœ“ å®ä¾‹æ³¨å†ŒæˆåŠŸ: {instance_id}")

        # æµ‹è¯•å®ä¾‹è·å–
        instance = instance_manager.get_instance(instance_id)
        assert instance is not None
        print(f"âœ“ å®ä¾‹è·å–æˆåŠŸ: {instance.collector.name}")

        # æµ‹è¯•çŠ¶æ€æ›´æ–°
        success = instance_manager.update_instance_stats(instance_id, True)
        assert success
        print(f"âœ“ å®ä¾‹çŠ¶æ€æ›´æ–°æˆåŠŸ")

        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = instance_manager.get_stats()
        print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯: æ´»è·ƒå®ä¾‹æ•° {stats['active_instances']}")

        # æµ‹è¯•å®ä¾‹åˆ—è¡¨
        all_instances = instance_manager.list_all_instances()
        assert len(all_instances) > 0
        print(f"âœ“ å®ä¾‹åˆ—è¡¨è·å–æˆåŠŸï¼Œå…± {len(all_instances)} ä¸ªå®ä¾‹")

        print("âœ“ å®ä¾‹ç®¡ç†å™¨å¢å¼ºéªŒè¯é€šè¿‡")

    except Exception as e:
        print(f"âœ— å®ä¾‹ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        raise


def test_enhanced_pipeline():
    """æµ‹è¯•å¢å¼ºçš„ç®¡é“å¤„ç†"""
    print("\n=== æµ‹è¯•å¢å¼ºçš„ç®¡é“å¤„ç† ===")

    try:
        # æµ‹è¯•å¢å¼ºçš„å»é‡é˜¶æ®µ
        dedup_stage = DedupStage(max_size=100, use_content_hash=True)

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        events = [
            RawEvent("test.source", {"post_id": "1", "content": "Content 1"}),
            RawEvent("test.source", {"post_id": "2", "content": "Content 2"}),
            RawEvent("test.source", {"post_id": "1", "content": "Content 1 duplicate"}),
            RawEvent(
                "test.source", {"url": "http://example.com", "content": "URL content"}
            ),
            RawEvent(
                "test.source", {"url": "http://example.com", "content": "URL duplicate"}
            ),
        ]

        # æµ‹è¯•å»é‡å¤„ç†
        result = dedup_stage.process(events)
        print(f"âœ“ å¢å¼ºå»é‡æµ‹è¯•ï¼šè¾“å…¥ {len(events)} ä¸ªï¼Œè¾“å‡º {len(result)} ä¸ª")

        # æµ‹è¯•æŒ‡æ ‡æ”¶é›†
        metrics = dedup_stage.get_metrics()
        print(
            f"âœ“ æŒ‡æ ‡æ”¶é›†: å¤„ç†æ•° {metrics['processed_count']}ï¼Œå»é‡æ•° {metrics['duplicates_found']}"
        )
        print(f"  - å»é‡ç‡: {metrics['dedup_rate']:.2%}")
        print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {metrics['cache_hit_rate']:.2%}")

        # æµ‹è¯•é”™è¯¯å¤„ç†
        invalid_events = [RawEvent("", {})]
        result_with_error = dedup_stage.process(invalid_events)
        print(f"âœ“ é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")

        print("âœ“ å¢å¼ºçš„ç®¡é“å¤„ç†éªŒè¯é€šè¿‡")

    except Exception as e:
        print(f"âœ— å¢å¼ºç®¡é“å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        raise


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹é‡æ„æ•ˆæœéªŒè¯æµ‹è¯•...")

    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_scheduler_strategy_pattern()
        test_pipeline_optimization()
        test_config_management()
        test_instance_manager()
        test_enhanced_pipeline()

        print("\nğŸ‰ æ‰€æœ‰é‡æ„éªŒè¯æµ‹è¯•é€šè¿‡ï¼")
        print("\né‡æ„æ•ˆæœæ€»ç»“ï¼š")
        print("âœ“ è°ƒåº¦å™¨ä½¿ç”¨ç­–ç•¥æ¨¡å¼ç®€åŒ–äº†å¤æ‚çš„æ¡ä»¶åˆ†æ”¯é€»è¾‘")
        print("âœ“ ç®¡é“å¤„ç†ä½¿ç”¨é›†åˆä¼˜åŒ–äº†å»é‡ç®—æ³•æ€§èƒ½ï¼ˆO(1) vs O(n)ï¼‰")
        print("âœ“ PipelineStageåŸºç±»æä¾›äº†ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’ŒæŒ‡æ ‡æ”¶é›†")
        print("âœ“ èŒè´£åˆ†ç¦»ä½¿ä»£ç æ›´æ¨¡å—åŒ–ã€å¯æµ‹è¯•å’Œå¯ç»´æŠ¤")
        print("âœ“ é‡‡é›†å™¨èƒ½åŠ›æ¥å£æå‡äº†ç±»å‹å®‰å…¨æ€§ï¼Œå‡å°‘è¿è¡Œæ—¶æ£€æŸ¥")
        print("âœ“ é…ç½®ç®¡ç†ä½¿ç”¨ç­–ç•¥æ¨¡å¼æ”¯æŒå¤šé…ç½®æºå’ŒéªŒè¯æœºåˆ¶")
        print("âœ“ å®ä¾‹ç®¡ç†å™¨æ”¯æŒçº¿ç¨‹å®‰å…¨ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œå¥åº·ç›‘æ§")
        print("âœ“ æ‰€æœ‰æ¨¡å—éƒ½å…·å¤‡å®Œå–„çš„é”™è¯¯å¤„ç†å’ŒæŒ‡æ ‡ç›‘æ§èƒ½åŠ›")

    except Exception as e:
        print(f"\nâŒ é‡æ„éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
